\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{float}
\usepackage{indentfirst}

\renewcommand{\topfraction}{0.9}
\pagestyle{fancy}
\fancyhf{}
\rhead{Kathryn Garside \and Pedro Pinto da Silva}
\lhead{Statistics for Big Data Project}
\rfoot{Page \thepage}

\begin{document}

\title{Marketing Dataset Analysis \\ MAS8381}
\author{Kathryn Garside \and Pedro Pinto da Silva}

\maketitle

%%  Libraries / hidden code

<<echo=FALSE, warning=FALSE, message=FALSE>>=
library(ElemStatLearn)
library(reshape2)
library(ggplot2)
library(MASS)
library(leaps)
library(glmnet)

data(marketing)
set.seed(1337)
@


%%

\section{Introduction and Exploratory Analysis}

In this paper we build and compare several models for predicting Income within the $marketing$ dataset. Both frequentist and bayesian methods are considered and put into practice. Although the main objective is to produce a model that can be used for prediction, we also examine models that hold higher interpretability and therefore are better suited for performing inference.

\subsection{Pre-processing}

The dataset is composed exclusively of numeric categorical variables. However, some are ordered and can be treated as continous while others are nominal and need to be converted to (unordered) factors as it does not make sense to treat them as continuous variables. For instance, Age is represented by discrete values in the range 1 through 7, where each value corresponds to an age interval. Therefore a decimal value for instance of 3.4 would be acceptable and representable within the spectrum of possible ages. On the other hand, Sex is represented by values 1 (Male) or 2 (Female) and a decimal value that lies between those does not make sense.

<<echo=FALSE>>=
marketing$Sex = factor(marketing$Sex,
                       labels = c("Male", "Female"),
                       ordered = FALSE)

marketing$Marital = factor(marketing$Marital,
                           labels = c("Married", "Living",
                                      "Divorced", "Widowed",
                                      "Single"),
                           ordered = FALSE)

marketing$Occupation = factor(marketing$Occupation,
                              labels = c("Professional", "Sales",
                                         "Factory", "Clerical",
                                         "Homemaker", "Student",
                                         "Military", "Retired",
                                         "Unemployed"),
                              ordered = FALSE)

marketing$Dual_Income = factor(marketing$Dual_Income,
                               labels = c("Not Married",
                                          "Yes", "No"),
                               ordered = FALSE)

marketing$Status = factor(marketing$Status,
                          labels = c("Own", "Rent",
                                     "WithFamily"),
                          ordered = FALSE)

marketing$Home_Type = factor(marketing$Home_Type,
                             labels = c("House", "Condo",
                                        "Apartment", "Mobile",
                                        "Home"),
                             ordered = FALSE)

marketing$Ethnic = factor(marketing$Ethnic,
                          labels = c("American Indian", "Asian",
                                     "Black", "East Indian", "Hispanic",
                                     "Pacific", "White", "Other"),
                          ordered = FALSE)

marketing$Language = factor(marketing$Language,
                            labels = c("English", "Spanish", "Other"),
                            ordered = FALSE)
@

\bigskip
In total we factored 8 out of 14 predictors: \{Sex, Marital, Occupation, Dual Income, Status, Home Type, Ehtnic, Language\}.
The data is structured as follows:
<<size="small">>=
str(marketing)
@

\subsubsection*{Missing Values Inputation}

A simple approach is to simply remove rows that contain at least a single missing predictor.
<<echo=FALSE>>=
marketingRaw = marketing
@
<<size="small">>=
# Remove row if it contains any missing predictor
marketing = marketing[complete.cases(marketing),]
@

However, by doing this we discard about 25\% of our dataset:
<<echo=FALSE>>=
paste("Updated number of rows:", nrow(marketing))
paste("Raw/Updated number of rows (%): ", round(nrow(marketing)/nrow(marketingRaw)*100,1), "%")
@

Therefore, alternative methods should be considered.
% Such as, blablabla .. apply them

\subsection{Summary Statistics}

As we are trying to predict annual income, we can start by observing how the data that on income is distributed:

<<echo=FALSE, out.width='5in', fig.align="center">>=
#ggplot(marketing, aes(x=Income)) +
#  geom_histogram(bins=50) + scale_x_continuous(limits=c(0,10), breaks = 1:9) +
#  ggtitle("Distribution of Annual Income of Household")
hist(marketing$Income, main="Distribution of Annual Income of Household", xla="Income", col="gray")
@

We could create pair plots for predictors, but 

<<size="small">>=
summary(marketing)
@


\section{Frequentist Analysis}

We start by splitting the data into train and test sets:

<<size="small">>=
x=model.matrix(Income~.,-1,data=marketing)[,-1]
y=marketing$Income

train=sample(1:nrow(x), nrow(x)*0.5)
test=(-train)
@


PCR
<<>>=

@

Lasso can be used for variable selection. 
<<size="small">>=
# Fitting a generalized model via penalized maximum likelihood
lasso.model = glmnet(x, y, alpha=1, lambda=10^seq(10, -2, length=100))
# Finding best lambda
cv.out = cv.glmnet(x[train,],y[train],alpha=1)
bestlam = cv.out$lambda.min
# Using model on test set
lasso.pred = predict(lasso.model, s=bestlam, newx = x[test,])
lasso.mse = mean((lasso.pred - y[test])^2)
lasso.coef = predict(lasso.model, type="coefficients", s=bestlam)

#plot(cv.out,main="Lasso")

lasso.mse
lasso.coef
@

Inspecting the residuals of the predicted values linear model with coefficients generated by lasso:

<<size="small">>=
residuals = lasso.pred - y[test]

par(mfrow = c(2,2))
hist(residuals)
qqnorm(residuals)
hist(y[test], main="Histogram of Income (test set)", xlab = "Income")
hist(lasso.pred, main="Histogram of Income (predicted)", xlab = "Income")
@


As our output is ordered we may think about apply an ordered logistic regression model.
<<cache=TRUE, size="small">>=
m <- polr(as.ordered(as.ordered(y)) ~ x, data = marketing, Hess=TRUE)
summary(m)
confint(m)
@
<<echo=FALSE>>=

@


% Explain which methods are applied and why

% Plot

% Choose Model

\section{Bayesian Analysis}

% Explain which methods are applied and why

% Plot

% Choose Model
We first use Gibbs Sampling to generate posterior distributions for coefficients of all the predictors. This was achieved using the following model in rJAGS,
<<size="small">>=
modelstring="model{
  for(i in 1:n){
    Y[i]~dnorm(Ymean[i],tau)
    Ymean[i]<-beta0+inprod(X[i,],beta)
  }
  tau~dgamma(1,0.001)
  beta0~dnorm(0,0.0001)
  for(j in 1:p){
    beta[j]~dnorm(0,0.001)
  }
}
"
@

\section{Discussion}

% Comparison of Frequentist and Bayesian approaches

\end{document}