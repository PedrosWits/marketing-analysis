\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{float}
\usepackage{indentfirst}

\renewcommand{\topfraction}{0.9}
\pagestyle{fancy}
\fancyhf{}
\rhead{Kathryn Garside \and Pedro Pinto da Silva}
\lhead{Statistics for Big Data Project}
\rfoot{Page \thepage}

\begin{document}

\title{Marketing Dataset Analysis \\ MAS8381}
\author{Kathryn Garside \and Pedro Pinto da Silva}

\maketitle

%%  Libraries / hidden code

<<echo=FALSE, warning=FALSE, message=FALSE>>=
library(ElemStatLearn)
library(reshape2)
library(ggplot2)
library(MASS)
library(leaps)
library(glmnet)

data(marketing)
set.seed(1337)
@


%%

\section{Introduction and Exploratory Analysis}

In this paper we build and compare several models for predicting Income within the $marketing$ dataset. Both frequentist and bayesian methods are considered and put into practice. Although the main objective is to produce a model that can be used for prediction, we also examine models that hold higher interpretability and therefore are better suited for performing inference.

\subsection{Pre-processing}

The dataset is composed exclusively of numeric categorical variables. However, some are ordered and can be treated as continous while others are nominal and need to be converted to (unordered) factors as it does not make sense to treat them as continuous variables. For instance, Age is represented by discrete values in the range 1 through 7, where each value corresponds to an age interval. Therefore a decimal value for instance of 3.4 would be acceptable and representable within the spectrum of possible ages. On the other hand, Sex is represented by values 1 (Male) or 2 (Female) and a decimal value that lies between those does not make sense.

<<echo=FALSE>>=
marketing$Sex = factor(marketing$Sex,
                       labels = c("Male", "Female"),
                       ordered = FALSE)

marketing$Marital = factor(marketing$Marital,
                           labels = c("Married", "Living",
                                      "Divorced", "Widowed",
                                      "Single"),
                           ordered = FALSE)

marketing$Occupation = factor(marketing$Occupation,
                              labels = c("Professional", "Sales",
                                         "Factory", "Clerical",
                                         "Homemaker", "Student",
                                         "Military", "Retired",
                                         "Unemployed"),
                              ordered = FALSE)

marketing$Dual_Income = factor(marketing$Dual_Income,
                               labels = c("Not Married",
                                          "Yes", "No"),
                               ordered = FALSE)

marketing$Status = factor(marketing$Status,
                          labels = c("Own", "Rent",
                                     "WithFamily"),
                          ordered = FALSE)

marketing$Home_Type = factor(marketing$Home_Type,
                             labels = c("House", "Condo",
                                        "Apartment", "Mobile",
                                        "Home"),
                             ordered = FALSE)

marketing$Ethnic = factor(marketing$Ethnic,
                          labels = c("American Indian", "Asian",
                                     "Black", "East Indian", "Hispanic",
                                     "Pacific", "White", "Other"),
                          ordered = FALSE)

marketing$Language = factor(marketing$Language,
                            labels = c("English", "Spanish", "Other"),
                            ordered = FALSE)
@

\bigskip
In total we factored 8 out of 14 predictors: \{Sex, Marital, Occupation, Dual Income, Status, Home Type, Ehtnic, Language\}.
The data is structured as follows:
<<size="small">>=
str(marketing)
@

\subsubsection*{Missing Values Inputation}

A simple approach is to simply remove rows that contain at least a single missing predictor.
<<echo=FALSE>>=
marketingRaw = marketing
@
<<size="small">>=
# Remove row if it contains any missing predictor
marketing = marketing[complete.cases(marketing),]
@

However, by doing this we discard about 25\% of our dataset:
<<echo=FALSE>>=
paste("Updated number of rows:", nrow(marketing))
paste("Raw/Updated number of rows (%): ", round(nrow(marketing)/nrow(marketingRaw)*100,1), "%")
@

Therefore, alternative methods should be considered.
% Such as, blablabla .. apply them

\subsection{Summary Statistics}

As we are trying to predict annual income, we can start by observing how the data that on income is distributed:

<<echo=FALSE, out.width='5in', fig.align="center">>=
#ggplot(marketing, aes(x=Income)) +
#  geom_histogram(bins=50) + scale_x_continuous(limits=c(0,10), breaks = 1:9) +
#  ggtitle("Distribution of Annual Income of Household")
hist(marketing$Income, main="Distribution of Annual Income of Household", xla="Income", col="gray")
@

We could create pair plots for predictors, but 

<<size="small">>=
summary(marketing)
@


\section{Frequentist Analysis}

We start by splitting the data into train and test sets:

<<size="small">>=
x=model.matrix(Income~.,-1,data=marketing)[,-1]
y=marketing$Income

train=sample(1:nrow(x), nrow(x)*0.5)
test=(-train)
@


PCR
<<>>=

@

Lasso can be used for variable selection. 
<<size="small">>=
# Fitting a generalized model via penalized maximum likelihood
lasso.model = glmnet(x, y, alpha=1, lambda=10^seq(10, -2, length=100))
# Finding best lambda
cv.out = cv.glmnet(x[train,],y[train],alpha=1)
bestlam = cv.out$lambda.min
# Using model on test set
lasso.pred = predict(lasso.model, s=bestlam, newx = x[test,])
lasso.mse = mean((lasso.pred - y[test])^2)
lasso.coef = predict(lasso.model, type="coefficients", s=bestlam)

#plot(cv.out,main="Lasso")

lasso.mse
lasso.coef
@

Inspecting the residuals of the predicted values linear model with coefficients generated by lasso:

<<size="small">>=
residuals = lasso.pred - y[test]

par(mfrow = c(2,2))
hist(residuals)
qqnorm(residuals)
hist(y[test], main="Histogram of Income (test set)", xlab = "Income")
hist(lasso.pred, main="Histogram of Income (predicted)", xlab = "Income")
@


As our output is ordered we may think about apply an ordered logistic regression model.
<<cache=TRUE, size="small">>=
m <- polr(as.ordered(as.ordered(y)) ~ x, data = marketing, Hess=TRUE)
summary(m)
confint(m)
@
<<echo=FALSE>>=

@


% Explain which methods are applied and why

% Plot

% Choose Model

\section{Bayesian Analysis}

% Explain which methods are applied and why
In order to compare our Bayesian results with the Frequentist approach we use a training set to build the model and use this to make predictions using a test set. The error between the prediction and actual result can then be computed and compared between the different approaches.\\

% Plot

% Choose Model
We first use Gibbs Sampling to generate posterior distributions for coefficients of all the predictors. This was achieved using the following model in RJAGS with 10000 iterations using the training data set,
<<size="small">>=
mf=model.frame(Income~.,data=marketing[train,])
Y=model.response(mf)
X=model.matrix(Income~.,mf)
n=nrow(X)
p=ncol(X)
data=list(n=n,p=p,Y=Y,X=X)
init=list(tau=1,beta=rep(0,p))
modelstring="model{
  for(i in 1:n){
    Y[i]~dnorm(Ymean[i],tau)
    Ymean[i]<-beta0+inprod(X[i,],beta)
  }
  tau~dgamma(1,0.001)
  beta0~dnorm(0,0.0001)
  for(j in 1:p){
    beta[j]~dnorm(0,0.001)
  }
}
"
@


<<echo=FALSE, warning=FALSE, message=FALSE>>=
set.seed(1337)
train=sample(1:nrow(marketing), nrow(marketing)*0.5)
test=(-train)
gibbsModel<-readRDS(paste(getwd(),"/TRAIN_GibbsSamplingMatrix.rdata",sep=""))
testMF=model.frame(Income~.,data=marketing[test,])
Y=model.response(testMF)
X=model.matrix(Income~.,testMF)

gibbsPredict=function(modelOutput,x){
  modelMatrix=as.matrix(modelOutput)
  modelCoeff=as.vector(colMeans(modelMatrix))
  beta0=modelCoeff[ncol(modelMatrix)-1]
  beta=modelCoeff[1:(ncol(modelMatrix)-2)]
  y=numeric(nrow(x))
  y=beta0+x%*%beta
}
@

The posterior distributions can then be used to make predictions using the test set. The error between these predictions and the actual result is calculated as $4.15065$.\\
<<size="small">>=
marketingGibbsPredict=gibbsPredict(gibbsModel,X)
mean((marketingGibbsPredict-marketing[test,]$Income)^2)
@

But as in the frequentist Lasso method, not all predictors need to be included. Next we use Bayesian variable selection with random effects and a prior on the inclusion probability to build a model that does not necessarily include all predictors. This was achieved using the following model with 10000 iterations on the training set,

<<size="small">>=
mf=model.frame(Income~.,data=marketing[train,])
Y=model.response(mf)
X=model.matrix(Income~.,mf)
n=nrow(X)
p=ncol(X)
data=list(n=n,p=p,Y=Y,X=X)
init=list(tau=1,beta=rep(0,p))
modelstring="model{
  for(i in 1:n){
    Y[i]~dnorm(Ymean[i],tau)
    Ymean[i]<-beta0+inprod(X[i,],beta)
  }
  tau~dgamma(1,0.001)
  beta0~dnorm(0,0.0001)
  for(j in 1:p){
  beta[j]~dnorm(0,0.001)
  }
}
"
@

<<echo=FALSE, warning=FALSE, message=FALSE>>=
set.seed(1337)
train=sample(1:nrow(marketing), nrow(marketing)*0.5)
test=(-train)
gibbsVarSelectModel<-readRDS(paste(getwd(),"/TRAIN_GibbsVariableSelectionMatrix.rdata",sep=""))
testMF=model.frame(Income~.,data=marketing[test,])
Y=model.response(testMF)
X=model.matrix(Income~.,testMF)
n=nrow(X)
p=ncol(X)

gibbsVarSelectPredict=function(modelOutput,x){
  modelMatrix=as.matrix(modelOutput)
  modelCoeff=as.vector(colMeans(modelMatrix))
  alpha=modelCoeff[1]
  beta=modelCoeff[2:37]
  ind=modelCoeff[38:73]
  pind=modelCoeff[74]
  tau=modelCoeff[75]
  taub=modelCoeff[76]
  y=numeric(nrow(x))
  y=alpha+x%*%(beta*ind)
}
@

<<echo=FALSE, warning=FALSE, message=FALSE>>=
inds=round(colMeans(gibbsVarSelectModel))[38:73]
zeroNames=NULL
nonzeroNames=NULL
for(i in 1:36){
  if(inds[i]==0){
    zeroNames=rbind(zeroNames,colnames(X)[i])
  }
  else{
    nonzeroNames=rbind(nonzeroNames,colnames(X)[i])
  }
}
@

The predictors included in the model selected by this method are,

<<size="small">>=
nonzeroNames[,1]
@

The predictors not included in the model are,

<<size="small">>=
zeroNames[,1]
@


The model built using this method can be used to make predictions using the test set. The difference between these predictions and the actual results is calculated as $4.198349$.

<<size="small">>=
marketingGibbsVarSelectPredict=gibbsVarSelectPredict(gibbsVarSelectModel,X)
mean((marketingGibbsVarSelectPredict-marketing[test,]$Income)^2)
@



\section{Discussion}

% Comparison of Frequentist and Bayesian approaches

\end{document}