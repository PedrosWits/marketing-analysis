\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{float}

\setlength{\parindent}{0cm}

\renewcommand{\topfraction}{0.9}
\pagestyle{fancy}
\fancyhf{}
\rhead{Kathryn Garside \and Pedro Pinto da Silva}
\lhead{Statistics for Big Data Project}
\rfoot{Page \thepage}

\begin{document}

\title{Marketing Dataset Analysis \\ MAS8381}
\author{Kathryn Garside \and Pedro Pinto da Silva}

\maketitle

%%  Libraries / hidden code

<<echo=FALSE, warning=FALSE, message=FALSE>>=
library(ElemStatLearn)
library(reshape2)
library(ggplot2)
library(MASS)
library(leaps)
library(glmnet)
library(corrplot)
library(pls)

data(marketing)
marketingRaw = marketing
set.seed(1337)
@


%%

\section{Introduction and Exploratory Analysis}

In this paper we build and compare several models for predicting Income within the $marketing$ dataset. Both frequentist and bayesian methods are considered and put into practice. Although the main objective is to produce a model that can be used for prediction, we also examine models that hold higher interpretability and therefore are better suited for performing inference.

\subsection{Pre-processing}

The dataset is composed exclusively of numeric categorical variables. However, some are ordered and can be treated as continous while others are nominal and need to be converted to (unordered) factors as it does not make sense to treat them as continuous variables. For instance, Age is represented by discrete values in the range 1 through 7, where each value corresponds to an age interval. Therefore a decimal value for instance of 3.4 would be acceptable and representable within the spectrum of possible ages. On the other hand, Sex is represented by values 1 (Male) or 2 (Female) and a decimal value that lies between those does not make sense.

<<echo=FALSE>>=
marketing$Sex = factor(marketing$Sex,
                       labels = c("Male", "Female"),
                       ordered = FALSE)

marketing$Marital = factor(marketing$Marital,
                           labels = c("Married", "Living",
                                      "Divorced", "Widowed",
                                      "Single"),
                           ordered = FALSE)

marketing$Occupation = factor(marketing$Occupation,
                              labels = c("Professional", "Sales",
                                         "Factory", "Clerical",
                                         "Homemaker", "Student",
                                         "Military", "Retired",
                                         "Unemployed"),
                              ordered = FALSE)

marketing$Dual_Income = factor(marketing$Dual_Income,
                               labels = c("Not Married",
                                          "Yes", "No"),
                               ordered = FALSE)

marketing$Status = factor(marketing$Status,
                          labels = c("Own", "Rent",
                                     "WithFamily"),
                          ordered = FALSE)

marketing$Home_Type = factor(marketing$Home_Type,
                             labels = c("House", "Condo",
                                        "Apartment", "Mobile",
                                        "Home"),
                             ordered = FALSE)

marketing$Ethnic = factor(marketing$Ethnic,
                          labels = c("American Indian", "Asian",
                                     "Black", "East Indian", "Hispanic",
                                     "Pacific", "White", "Other"),
                          ordered = FALSE)

marketing$Language = factor(marketing$Language,
                            labels = c("English", "Spanish", "Other"),
                            ordered = FALSE)
@

\bigskip
In total we factored 8 out of 14 predictors: \{Sex, Marital, Occupation, Dual Income, Status, Home Type, Ehtnic, Language\}. After factoring the relevant predictors, the data is structured as follows:
<<size="small">>=
str(marketing)
@

\subsubsection*{Missing Values Imputation}

A simple approach is to simply remove rows that contain at least a single missing predictor.
<<echo=FALSE>>=
marketingRaw = marketingRaw[complete.cases(marketingRaw),]
@


<<size="small">>=
# Remove row if it contains any missing predictor
marketing = marketing[complete.cases(marketing),]
@

However, by doing this we discard about 25\% of our dataset:
<<echo=FALSE>>=
paste("Updated number of rows:", nrow(marketing))
paste("Raw/Updated number of rows (%): ", round(nrow(marketing)/nrow(marketingRaw)*100,1), "%")
@

Therefore, alternative methods should be considered.
% Such as, blablabla .. apply them

\subsection{Summary Statistics}

As we are trying to predict annual income, we can start by observing how the data on income is distributed.
<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Histogram of Income and distribution of Income against Sex.', fig.align="center", warning=FALSE>>=
ggplot(marketing, aes(x=Income)) +
  geom_histogram(bins=35) + scale_x_continuous(limits=c(0,10), breaks = 1:9)
ggplot(marketing, aes(y=Income, x=Sex)) + 
  geom_boxplot(aes(fill=Sex), width=0.6)
@

We can also look at how Income varies against other predictors by creating independent boxplots. We can observe that some predictors seem to contain noticeable difference between Incomes for available categories.

\bigskip
We can check for redundancy between co-variates by computing and plotting the correlation matrix. However, computing Pearson's or Spearman's correlation between a nominal and a continuous numeric value is not appropriate. Therefore, we only compute the correlation between predictors that can be considered numeric. There are more suitable methods to test the independence or measure the association between two nominal or one nominal and numeric co-variates, such as the Chi-Square test and Crammer's V measure of correlation. Yet, we do not cover those here.

<<echo=FALSE, size="small",fig.cap='Income against predictors and correlation plot of numeric variables.', fig.align="center", warning=FALSE, out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H'>>=
ggplot(data = melt(marketingRaw[, -2], id.var = "Income"), 
       aes(y=Income, x=value)) + 
  geom_boxplot(aes(group=value)) +
  facet_wrap(~ variable, scales="free")

corrplot:::corrplot(cor(marketing[, sapply(marketing, class) != "factor"]))
@


\section{Frequentist Analysis}

In this section we try to present a generalized linear model built from a frequentist approach. We will start by applying methods that try to reduce the dimensionality of the problem and perform variable selection, in order to understand which predictors are more relevant than others and detect if any can be discarded.

We start by splitting the data into train and test sets and performing principal component analysis. This enables us to understand how many components are required to explain the majority of variance in the data.
<<size="small">>=
x=model.matrix(Income~.,-1,data=marketing)[,-1]
y=marketing$Income

train=sample(1:nrow(x), nrow(x)*0.5)
test=(-train)

pcr.fit = pcr(Income~., data=marketing[train, ], scale=TRUE, validation="CV")
# Number of components necessary to explain at least 80% of variance
pcr.comp.80 = pcr.fit$ladings[,which(pcr.fit$importance[3,] >= 0.80)]

# Running model on test set
pcr.pred = predict(pcr.fit, x[test,], ncomp=32)
pcr.mse = mean((pcr.pred-y[test])^2)

paste("Number of components (80% var) =", pcr.comp.80)
paste("MSE on test set =", pcr.mse)
@

Alternatively, Lasso can be used for variable selection and produce a generalized linear model where less important variables are shrunk towards zero.
<<size="small", out.width='4.5in', fig.align="center">>=
# Fitting a generalized model via penalized maximum likelihood
lasso.model = glmnet(x, y, alpha=1, lambda=10^seq(10, -2, length=100))
# Finding best lambda
cv.out = cv.glmnet(x[train,],y[train],alpha=1)
bestlam = cv.out$lambda.min
# Using model on test set
lasso.pred = predict(lasso.model, s=bestlam, newx = x[test,])
lasso.mse = mean((lasso.pred - y[test])^2)
lasso.coef = predict(lasso.model, type="coefficients", s=bestlam)
lasso.residuals = lasso.pred - y[test]

paste("MSE test for g.l. model fitted via Lasso =", lasso.mse)
@

Looking at the plots produced by both methods we can denote some degree of agreeability between the two, as both require more than 30 components/co-variates in order to minimize the MSE (down to value approximately equal to 4).
<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Introspection plots for lasso and pcr.', fig.align="center">>=
plot(cv.out, main="Lasso")
validationplot(pcr.fit, val.type="MSEP", main="Pcr")
@

Still, is of interest to analyse the residuals to see if these provide a good represention of the random and unpredictable characteristics of the error term.
<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.align="center">>=
hist(lasso.residuals, main = "", xlab="")
qqnorm(lasso.residuals, main = "")
@

By inspecting the distribution of residuals and associated QQ plot, we can observe that the residuals are reasonably well behaved, i.e. normally distributed and centered around zero. At first this may indicate that the choice of model is appropriate, however by

<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Plots for inspecting residuals: Histogram of residuals, QQ plot and residuals plotted against true and fitted values of Income.', fig.align="center">>=
plot(x = y[test], y = lasso.residuals, xlab="Test values", ylab="Residuals")
plot(x = lasso.pred, y = lasso.residuals, xlab="Fitted values", ylab="Residuals")
@

<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Histograms for true and predicted values of Income'>>=
hist(y[test], main="True", xlab = "Income", col="purple")
hist(lasso.pred, main="Predicted", xlab = "Income", col="gray")
@



Given that our output is ordered we may think about applying ordered logistic regression.
<<cache=TRUE, size="small", message=FALSE, warning=FALSE>>=
olr.model = polr(as.ordered(as.ordered(y)) ~ x, data = marketing, Hess=TRUE)
olr.cis = confint(olr.model)
olr.predict = predict(olr.model, x)
@

<<echo=FALSE>>=
olr.cis.ratio = olr.cis[,1]/olr.cis[,2]
names(olr.cis.ratio) = rownames(olr.cis)
olr.cis.ratio = sort(olr.cis.ratio)
olr.cis.ratio.belowZero = olr.cis.ratio[olr.cis.ratio <= 0]
olr.cis.ratio.greaterZero = olr.cis.ratio[olr.cis.ratio >= 0]
nBelowZero = length(olr.cis.ratio.belowZero)
nAboveZero = length(olr.cis.ratio.greaterZero)

ggplot() +
  geom_point(data=melt(olr.cis.ratio.greaterZero), 
             aes(y=value, x=(nBelowZero+1):(nBelowZero+nAboveZero)), colour="cadetblue3", size=2) +
  geom_point(data=melt(olr.cis.ratio.belowZero), 
             aes(y=value, x=1:length(olr.cis.ratio.belowZero), 
                 color=names(olr.cis.ratio.belowZero)), size=2) +
  #geom_label(size=2.5, fontface="bold", fill="cadetblue3", colour="white") +
  geom_hline(yintercept=0) +
  xlab("Index") +
  ylab("Ratio between upper and lower CI") +
  theme_bw() +
  theme(legend.position="top", legend.text = element_text(size=6)) +
  labs(color="Ineffective Predictors")
@


% Choose Model

\section{Bayesian Analysis}

% Explain which methods are applied and why
In order to compare our Bayesian results with the Frequentist approach we use a training set to build the model and use this to make predictions using a test set. The error between the prediction and actual result can then be computed and compared between the different approaches.\\

% Plot

% Choose Model
We first use Gibbs Sampling to generate posterior distributions for coefficients of all the predictors. This was achieved using the following model in RJAGS with 10000 iterations using the training data set,
<<size="small">>=
mf=model.frame(Income~.,data=marketing[train,])
Y=model.response(mf)
X=model.matrix(Income~.,mf)
n=nrow(X)
p=ncol(X)
data=list(n=n,p=p,Y=Y,X=X)
init=list(tau=1,beta=rep(0,p))
modelstring="model{
  for(i in 1:n){
    Y[i]~dnorm(Ymean[i],tau)
    Ymean[i]<-beta0+inprod(X[i,],beta)
  }
  tau~dgamma(1,0.001)
  beta0~dnorm(0,0.0001)
  for(j in 1:p){
    beta[j]~dnorm(0,0.001)
  }
}
"
@


<<echo=FALSE, warning=FALSE, message=FALSE>>=
set.seed(1337)
train=sample(1:nrow(marketing), nrow(marketing)*0.5)
test=(-train)
gibbsModel<-readRDS(paste(getwd(),"/TRAIN_GibbsSamplingMatrix.rdata",sep=""))
testMF=model.frame(Income~.,data=marketing[test,])
Y=model.response(testMF)
X=model.matrix(Income~.,testMF)

gibbsPredict=function(modelOutput,x){
  modelMatrix=as.matrix(modelOutput)
  modelCoeff=as.vector(colMeans(modelMatrix))
  beta0=modelCoeff[ncol(modelMatrix)-1]
  beta=modelCoeff[1:(ncol(modelMatrix)-2)]
  y=numeric(nrow(x))
  y=beta0+x%*%beta
}
@

The posterior distributions can then be used to make predictions using the test set. The error between these predictions and the actual result is calculated as $4.15065$.\\
<<size="small">>=
marketingGibbsPredict=gibbsPredict(gibbsModel,X)
mean((marketingGibbsPredict-marketing[test,]$Income)^2)
@

But as in the frequentist Lasso method, not all predictors need to be included. Next we use Bayesian variable selection with random effects and a prior on the inclusion probability to build a model that does not necessarily include all predictors. This was achieved using the following model with 10000 iterations on the training set,

<<size="small">>=
mf=model.frame(Income~.,data=marketing[train,])
Y=model.response(mf)
X=model.matrix(Income~.,mf)
n=nrow(X)
p=ncol(X)
data=list(n=n,p=p,Y=Y,X=X)
init=list(tau=1,beta=rep(0,p))
modelstring="model{
  for(i in 1:n){
    Y[i]~dnorm(Ymean[i],tau)
    Ymean[i]<-beta0+inprod(X[i,],beta)
  }
  tau~dgamma(1,0.001)
  beta0~dnorm(0,0.0001)
  for(j in 1:p){
  beta[j]~dnorm(0,0.001)
  }
}
"
@

<<echo=FALSE, warning=FALSE, message=FALSE>>=
set.seed(1337)
train=sample(1:nrow(marketing), nrow(marketing)*0.5)
test=(-train)
gibbsVarSelectModel<-readRDS(paste(getwd(),"/TRAIN_GibbsVariableSelectionMatrix.rdata",sep=""))
testMF=model.frame(Income~.,data=marketing[test,])
Y=model.response(testMF)
X=model.matrix(Income~.,testMF)
n=nrow(X)
p=ncol(X)

gibbsVarSelectPredict=function(modelOutput,x){
  modelMatrix=as.matrix(modelOutput)
  modelCoeff=as.vector(colMeans(modelMatrix))
  alpha=modelCoeff[1]
  beta=modelCoeff[2:37]
  ind=modelCoeff[38:73]
  pind=modelCoeff[74]
  tau=modelCoeff[75]
  taub=modelCoeff[76]
  y=numeric(nrow(x))
  y=alpha+x%*%(beta*ind)
}
@

<<echo=FALSE, warning=FALSE, message=FALSE>>=
inds=round(colMeans(gibbsVarSelectModel))[38:73]
zeroNames=NULL
nonzeroNames=NULL
for(i in 1:36){
  if(inds[i]==0){
    zeroNames=rbind(zeroNames,colnames(X)[i])
  }
  else{
    nonzeroNames=rbind(nonzeroNames,colnames(X)[i])
  }
}
@

The predictors included in the model selected by this method are,

<<size="small">>=
nonzeroNames[,1]
@

The predictors not included in the model are,

<<size="small">>=
zeroNames[,1]
@


The model built using this method can be used to make predictions using the test set. The difference between these predictions and the actual results is calculated as $4.198349$.

<<size="small">>=
marketingGibbsVarSelectPredict=gibbsVarSelectPredict(gibbsVarSelectModel,X)
mean((marketingGibbsVarSelectPredict-marketing[test,]$Income)^2)
@



\section{Discussion}

% Comparison of Frequentist and Bayesian approaches

\end{document}