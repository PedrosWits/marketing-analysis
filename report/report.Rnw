\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{float}

\setlength{\parindent}{0cm}

\renewcommand{\topfraction}{0.9}
\pagestyle{fancy}
\fancyhf{}
\rhead{Kathryn Garside \and Pedro Pinto da Silva}
\lhead{Statistics for Big Data Project}
\rfoot{Page \thepage}

\begin{document}

\title{Marketing Dataset Analysis \\ MAS8381}
\author{Kathryn Garside \and Pedro Pinto da Silva}

\maketitle

%%  Libraries / hidden code

<<echo=FALSE, warning=FALSE, message=FALSE>>=
library(ElemStatLearn)
library(reshape2)
library(ggplot2)
library(MASS)
library(leaps)
library(glmnet)
library(corrplot)
library(pls)
library(VIM)
library(mice)

data(marketing)
marketingRaw = marketing
set.seed(1337)
@


%%

\section{Introduction and Exploratory Analysis}

In this paper we build and compare several models for predicting Income within the $marketing$ dataset. Both frequentist and bayesian methods are considered and put into practice. Although the main objective is to produce a model that can be used for prediction, we also take into account its interpretability and therefore how useful it is for performing inference.

\subsection{Pre-processing}

The dataset is composed exclusively of numeric variables which are categorical in nature. Some are ordered and can be treated as continous while others are nominal and need to be converted to (unordered) factors as it does not make sense to treat them as continuous variables. For instance, Age is represented by discrete values in the range 1 through 7, where each value corresponds to an age interval. Therefore a decimal value ,for instance of 3.4, would be acceptable and representable within the spectrum of possible ages. On the other hand, Sex is represented by values 1 (Male) or 2 (Female) and a decimal value that lies between those does not make sense.

<<echo=FALSE>>=
marketing$Sex = factor(marketing$Sex,
                       labels = c("Male", "Female"),
                       ordered = FALSE)

marketing$Marital = factor(marketing$Marital,
                           labels = c("Married", "Living",
                                      "Divorced", "Widowed",
                                      "Single"),
                           ordered = FALSE)

marketing$Occupation = factor(marketing$Occupation,
                              labels = c("Professional", "Sales",
                                         "Factory", "Clerical",
                                         "Homemaker", "Student",
                                         "Military", "Retired",
                                         "Unemployed"),
                              ordered = FALSE)

marketing$Dual_Income = factor(marketing$Dual_Income,
                               labels = c("Not Married",
                                          "Yes", "No"),
                               ordered = FALSE)

marketing$Status = factor(marketing$Status,
                          labels = c("Own", "Rent",
                                     "WithFamily"),
                          ordered = FALSE)

marketing$Home_Type = factor(marketing$Home_Type,
                             labels = c("House", "Condo",
                                        "Apartment", "Mobile",
                                        "Home"),
                             ordered = FALSE)

marketing$Ethnic = factor(marketing$Ethnic,
                          labels = c("American Indian", "Asian",
                                     "Black", "East Indian", "Hispanic",
                                     "Pacific", "White", "Other"),
                          ordered = FALSE)

marketing$Language = factor(marketing$Language,
                            labels = c("English", "Spanish", "Other"),
                            ordered = FALSE)
@

\bigskip
In total we factored 8 out of 14 predictors: \{Sex, Marital, Occupation, Dual Income, Status, Home Type, Ehtnic, Language\}. After factoring the relevant predictors, the data is structured as follows:
<<size="small">>=
str(marketing)
@

\subsubsection*{Missing Values Imputation}

A simple approach is to simply remove rows that contain at least a single missing predictor.
<<echo=FALSE>>=
marketingRaw = marketing
@
<<size="small">>=
# Remove row if it contains any missing predictor
marketing = marketing[complete.cases(marketing),]
@

However, by doing this we discard about 25\% of our dataset:
<<echo=FALSE>>=
paste("Updated number of rows:", nrow(marketing))
paste("Raw/Updated number of rows (%): ", round(nrow(marketing)/nrow(marketingRaw)*100,1), "%")
@

Some of the predictors have more missing values than others:
<<echo=FALSE, size="small",fig.cap='Missing Data', fig.align="center", warning=FALSE, out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H'>>=
aggr_plot <-aggr(marketingRaw, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(marketingRaw), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
@


Therefore, alternative methods should be considered. One alternative is to use the R package $mice$ to impute the missing data:
<<results='hide',message=FALSE,warning=FALSE,cache=TRUE,size="small">>=
imputedData <- mice(marketingRaw, m=10, maxit = 50, method = 'pmm', seed = 500)
marketingImputed=complete(imputedData,10)
@
In this report we have only used the entries in the $marketing$ data which have no missing fields. The analysis could easily be reproduced using the imputed dataset, however.
% Such as, blablabla .. apply them

\subsection{Summary Statistics}

As we are trying to predict annual income, we can start by observing how the data on income is distributed.
<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Histogram of Income and distribution of Income against Sex.', fig.align="center", warning=FALSE>>=
ggplot(marketing, aes(x=Income)) +
  geom_histogram(bins=35) + scale_x_continuous(limits=c(0,10), breaks = 1:9)
ggplot(marketing, aes(y=Income, x=Sex)) + 
  geom_boxplot(aes(fill=Sex), width=0.6)
@

We can also look at how Income varies against other predictors by creating independent boxplots. We can observe that some predictors seem to contain noticeable differences between Incomes for available categories.

\bigskip
We can check for redundancy between co-variates by computing and plotting the correlation matrix. However, computing Pearson's or Spearman's correlation between a nominal and a continuous numeric value is not appropriate. Therefore, we only compute the correlation between predictors that can be considered numeric. There are more suitable methods to test the independence or measure the association between two nominal or one nominal and numeric co-variates, such as the Chi-Square test and Crammer's V measure of correlation. Yet, we do not cover those here.

<<echo=FALSE, size="small",fig.cap='Income against predictors and correlation plot of numeric variables.', fig.align="center", warning=FALSE, out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H'>>=
ggplot(data = melt(marketingRaw[, -2], id.var = "Income"), 
       aes(y=Income, x=value)) + 
  geom_boxplot(aes(group=value)) +
  facet_wrap(~ variable, scales="free")

corrplot:::corrplot(cor(marketing[, sapply(marketing, class) != "factor"]))
@


\section{Frequentist Analysis}

In this section we try to develop a generalized linear model from a frequentist approach. We start by applying methods that try to reduce the dimensionality of the problem and perform variable selection, in order to understand which predictors are more relevant than others and detect if any can be discarded.
\bigskip
We start by splitting the data into train and test sets and performing principal component analysis. This enables us to understand how many components are required to explain the majority of variance in the data.
<<size="small">>=
x=model.matrix(Income~.,-1,data=marketing)[,-1]
y=marketing$Income

train=sample(1:nrow(x), nrow(x)*0.5)
test=(-train)

pcr.fit = pcr(Income~., data=marketing, subset=train, scale=TRUE, validation="CV")

# Find the number of components that minimizes TEST MSE
pred.pcr = predict(pcr.fit, ncomp = 1:35, newdata=marketing[test,])
pcr.mses = apply(pred.pcr, 3, function(i) mean((i- y[test])^2)) #

@
<<echo=FALSE>>=
paste("Min PCR MSE on test set =", round(min(pcr.mses), 2))
paste("Number of components =", which.min(pcr.mses))
@
As we can see, most components are necessary the explain the majority of variance. Furthermore the number of components that minimizes the MSE on the test set is close to the maximum number of components. Therefore, principal component analysis fails to simplify the problem and provide a considerable reduction in the dimensionality of the problem.
\bigskip
Alternatively, Lasso can be used for variable selection and produce a generalized linear model where less important variables are shrunk towards zero.
<<size="small", out.width='4.5in', fig.align="center">>=
# Fitting a generalized model via penalized maximum likelihood
lasso.model = glmnet(x[train,], y[train], alpha=1, lambda=10^seq(10, -2, length=100))
# Finding best lambda
cv.out = cv.glmnet(x[train,],y[train],alpha=1)
bestlam = cv.out$lambda.min
# Using model on test set
lasso.pred = predict(lasso.model, s=bestlam, newx = x[test,])
lasso.mse = mean((lasso.pred - y[test])^2)
lasso.coef = predict(lasso.model, type="coefficients", s=bestlam)
lasso.residuals = lasso.pred - y[test]
@
<<echo=FALSE>>=
paste("Number of coefficients zeroed out =", length(which(lasso.coef == 0)))
paste("Lasso MSE on test set =", round(lasso.mse,2))
@

We can see that only 3 out of 36 coefficients were actually zeroed out by Lasso. Moreover, it is usually good practice and of interest to analyse the residuals to check if these provide a good represention of the random and unpredictable characteristics of the error term.
<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.align="center">>=
hist(lasso.residuals, main = "", xlab="")
qqnorm(lasso.residuals, main = "")
@

By inspecting the distribution of residuals and associated QQ plot, we can observe that the residuals are reasonably well behaved, i.e. normally distributed and centered around zero. At first this may indicate that the choice of model is appropriate, however we also need to check for residuals independence, i.e. that are not correlated.

<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Plots for inspecting residuals: Histogram of residuals, QQ plot and residuals plotted against true and fitted values of Income.', fig.align="center">>=
plot(x = y[test], y = lasso.residuals, xlab="Test values", ylab="Residuals")
plot(x = lasso.pred, y = lasso.residuals, xlab="Fitted values", ylab="Residuals")
@

In fact, we can observe a slight pattern: while the residuals for some categories are centered around zero, they are not for others. This becomes apparent if we compare the histograms for predicted and true values. The model strongly under-predicts values from categories at the tails, including the most common category (with value 1), and over-predicts values in the middle categories. This indicates that even though MSE is being minimised, our model is not very accurate when it comes to placing values in the correct category.

<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Histograms for true and predicted values of Income -- Lasso'>>=
hist(y[test], main="True", xlab = "Income", col="purple")
hist(lasso.pred, main="Predicted", xlab = "Income", col="gray")
@

Looking at the results and plots produced by both methods we can denote some degree of agreeability between the two, as both require more than 30 components/co-variates in order to minimize the MSE on the test set. However, a generalized linear model with coefficients computed via Lasso, provides a more interpretable model and therefore easier to explain how the dependent variables affect the output. Hence, we consider the second model to be preferable to the first.

<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Introspection plots for lasso and pcr.', fig.align="center">>=
plot(cv.out, main="Lasso")
validationplot(pcr.fit, val.type="MSEP", main="Pcr")
@

Alternatively, given that our output is inherently ordered we may think about looking at extensions of the generalized linear model. Namely, we may think about applying ordered logistic regression, which treats the output as an ordered categorical variable instead of a continuous numeric variable (as Lasso did). To that extent, we fit a proportional odds model using the polr function:
<<cache=TRUE, size="small", message=FALSE, warning=FALSE>>=
marketing$Income = as.ordered(marketing$Income)
olr.model = polr(Income ~., data = marketing, subset= train, Hess=TRUE)
olr.cis = confint(olr.model)
olr.predict = predict(olr.model, marketing[test,])
@
<<echo=FALSE>>=
olr.residuals = as.numeric(olr.predict) - y[test]
olr.mse = mean((as.numeric(olr.predict) - y[test])^2)
paste("OLR MSE on test set =", round(olr.mse))
paste("Number of categories accurately predicted =", sum(olr.predict == y[test]))
@

Where we compute the 95\% confidence intervals for the estimated co-variates. We can test the significance of a variable by checking whether the confidence interval contains zero or not. If it does, then it means that the predictor can both contribute positively and negatively to the output. Which is not a very consistent or effective predictor. Therefore, such predictors should be discarded. We try to represent effectiveness of predictors by multiplying the lower and upper terms of the interval. If the result is negative then it means that the interval contains zero, otherwise it does not. These points are represented in the plot below. We can see that a substantial amount of predictors are ineffective.

<<echo=FALSE, out.width='14cm', out.height='14cm', fig.cap='Representation of the effectiveness of the estimated coefficients. Negative values indicate that the confidence interval contains zero and therefore is not an effective predictor.', fig.align="center">>=
marketing$Income = as.numeric(marketing$Income)

olr.cis.ratio = olr.cis[,1]*olr.cis[,2]
names(olr.cis.ratio) = rownames(olr.cis)
olr.cis.ratio = sort(olr.cis.ratio)
olr.cis.ratio.belowZero = olr.cis.ratio[olr.cis.ratio <= 0]
olr.cis.ratio.greaterZero = olr.cis.ratio[olr.cis.ratio >= 0]
nBelowZero = length(olr.cis.ratio.belowZero)
nAboveZero = length(olr.cis.ratio.greaterZero)

ggplot() +
  geom_point(data=melt(olr.cis.ratio.greaterZero), 
             aes(y=value, x=(nBelowZero+1):(nBelowZero+nAboveZero)), colour="cadetblue3", size=2) +
  geom_point(data=melt(olr.cis.ratio.belowZero), 
             aes(y=value, x=1:length(olr.cis.ratio.belowZero), 
                 color=names(olr.cis.ratio.belowZero)), size=2) +
  #geom_label(size=2.5, fontface="bold", fill="cadetblue3", colour="white") +
  geom_hline(yintercept=0) +
  xlab("Index") +
  ylab("Ratio") +
  theme_bw() +
  theme(legend.position="top", legend.text = element_text(size=6)) +
  labs(color="Ineffective Predictors")
@
\bigskip
Below we can observe the performance of the model at predicting the correct categories of the output. We see that the distribution of predicted values better resembles the true distribution when compared to Lasso. However, while it predicts the correct categories for the most frequent categories it fails to do so for the categories in between. In fact, it fails to assign values to certain categories altogether. The reason for why this happens should be further investigated.
<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Histograms for true and predicted values of Income -- OLR'>>=
hist(y[test], main="True", xlab = "Income", col="purple")
hist(as.numeric(olr.predict), main="Predicted", xlab = "Income", col="gray")
@
\bigskip
Taking into account the three different methodologies, we consider that the generalized linear model with coefficients estimated via Lasso is the more consistent choice. Even though that the model fails to accurately predict values from the more frequent categories, it contains the lowest MSE on the test set and is the most interpretable model of the three.

<<>>=
proposed.model = glmnet(x, y, alpha=1, lambda=10^seq(10, -2, length=100))
bestlam = cv.glmnet(x[train,],y[train],alpha=1)$lambda.min
proposed.coef = predict(proposed.model, type="coefficients", s=bestlam)
@


% Choose Model

\section{Bayesian Analysis}

% Explain which methods are applied and why
In order to compare our Bayesian results with the Frequentist approach we use a training set to build the model and use this to make predictions using a test set. The error between the prediction and actual result can then be computed and compared between the different approaches.\\

% Plot

% Choose Model
We first use Gibbs Sampling to generate posterior distributions for coefficients of all the predictors. This was achieved using the following model in RJAGS with 10000 iterations using the training data set,
<<size="small">>=
mf=model.frame(Income~.,data=marketing[train,])
Y=model.response(mf)
X=model.matrix(Income~.,mf)
n=nrow(X)
p=ncol(X)
data=list(n=n,p=p,Y=Y,X=X)
init=list(tau=1,beta=rep(0,p))
modelstring="model{
  for(i in 1:n){
    Y[i]~dnorm(Ymean[i],tau)
    Ymean[i]<-beta0+inprod(X[i,],beta)
  }
  tau~dgamma(1,0.001)
  beta0~dnorm(0,0.0001)
  for(j in 1:p){
    beta[j]~dnorm(0,0.001)
  }
}
"
@


<<echo=FALSE, warning=FALSE, message=FALSE>>=
set.seed(1337)
train=sample(1:nrow(marketing), nrow(marketing)*0.5)
test=(-train)
gibbsModel<-readRDS(paste(getwd(),"/TRAIN_GibbsSamplingMatrix.rdata",sep=""))
testMF=model.frame(Income~.,data=marketing[test,])
Y=model.response(testMF)
X=model.matrix(Income~.,testMF)

gibbsPredict=function(modelOutput,x){
  modelMatrix=as.matrix(modelOutput)
  modelCoeff=as.vector(colMeans(modelMatrix))
  beta0=modelCoeff[ncol(modelMatrix)-1]
  beta=modelCoeff[1:(ncol(modelMatrix)-2)]
  y=numeric(nrow(x))
  y=beta0+x%*%beta
}
@

The posterior distributions can then be used to make predictions using the test set. The error between these predictions and the actual result is calculated as $4.15065$.\\
<<echo=FALSE>>=
marketingGibbsPredict=gibbsPredict(gibbsModel,X)
gibbs.mse=mean((marketingGibbsPredict-marketing[test,]$Income)^2)
@

<<echo=FALSE>>=
paste("MSE test for ordered Gibbs Sampling model =", gibbs.mse)
@


<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Histograms for true and predicted values of Income using Gibbs Sampling'>>=
hist(y[test], main="True", xlab = "Income", col="purple")
hist(marketingGibbsPredict, main="Predicted", xlab = "Income", col="gray")
@

But as in the frequentist Lasso method, not all predictors need to be included. Next we use Bayesian variable selection with random effects and a prior on the inclusion probability to build a model that does not necessarily include all predictors. This was achieved using the following model with 10000 iterations on the training set,

<<size="small">>=
mf=model.frame(Income~.,data=marketing[train,])
Y=model.response(mf)
X=model.matrix(Income~.,mf)
n=nrow(X)
p=ncol(X)
data=list(n=n,p=p,Y=Y,X=X)
init=list(tau=1,beta=rep(0,p))
modelstring="model{
  for(i in 1:n){
    Y[i]~dnorm(Ymean[i],tau)
    Ymean[i]<-beta0+inprod(X[i,],beta)
  }
  tau~dgamma(1,0.001)
  beta0~dnorm(0,0.0001)
  for(j in 1:p){
  beta[j]~dnorm(0,0.001)
  }
}
"
@

<<echo=FALSE, warning=FALSE, message=FALSE>>=
set.seed(1337)
train=sample(1:nrow(marketing), nrow(marketing)*0.5)
test=(-train)
gibbsVarSelectModel<-readRDS(paste(getwd(),"/TRAIN_GibbsVariableSelectionMatrix.rdata",sep=""))
testMF=model.frame(Income~.,data=marketing[test,])
Y=model.response(testMF)
X=model.matrix(Income~.,testMF)
n=nrow(X)
p=ncol(X)

gibbsVarSelectPredict=function(modelOutput,x){
  modelMatrix=as.matrix(modelOutput)
  modelCoeff=as.vector(colMeans(modelMatrix))
  alpha=modelCoeff[1]
  beta=modelCoeff[2:37]
  ind=modelCoeff[38:73]
  pind=modelCoeff[74]
  tau=modelCoeff[75]
  taub=modelCoeff[76]
  y=numeric(nrow(x))
  y=alpha+x%*%(beta*ind)
}
@

<<echo=FALSE, warning=FALSE, message=FALSE>>=
inds=round(colMeans(gibbsVarSelectModel))[38:73]
zeroNames=NULL
nonzeroNames=NULL
for(i in 1:36){
  if(inds[i]==0){
    zeroNames=rbind(zeroNames,colnames(X)[i])
  }
  else{
    nonzeroNames=rbind(nonzeroNames,colnames(X)[i])
  }
}
@

The predictors included in the model selected by this method are,

<<size="small">>=
nonzeroNames[,1]
@

The predictors not included in the model are,

<<size="small">>=
zeroNames[,1]
@


The model built using this method can be used to make predictions using the test set. The difference between these predictions and the actual results is calculated as $4.198349$.

<<echo=FALSE>>=
marketingGibbsVarSelectPredict=gibbsVarSelectPredict(gibbsVarSelectModel,X)
gibbsVarSelect.mse=mean((marketingGibbsVarSelectPredict-marketing[test,]$Income)^2)
@

<<echo=FALSE>>=
paste("MSE test for ordered Gibbs Variable Selection model =", gibbsVarSelect.mse)
@

<<echo=FALSE, size="small", out.width='8cm', out.height='8cm', fig.show='hold', fig.pos='H', fig.cap='Histograms for true and predicted values of Income using Gibbs Sampling Variable Selection'>>=
hist(y[test], main="True", xlab = "Income", col="purple")
hist(marketingGibbsVarSelectPredict, main="Predicted", xlab = "Income", col="gray")
@

\section{Discussion}

% Comparison of Frequentist and Bayesian approaches
The MSE for each of the methods explored in this report are given below,
<<echo=FALSE>>=
paste("Method: PCR                       MSE=",min(pcr.mses))
paste("Method: Lasso                     MSE=",lasso.mse)
paste("Method: OLR                       MSE=",olr.mse)
paste("Method: Gibbs Sampling            MSE=",gibbs.mse)
paste("Method: Gibbs Variable Selection  MSE=",gibbsVarSelect.mse)
@
The methods PCR, Lasso, Gibbs Sampling and Gibbs Variable Selection all give very similar, and low, MSE results for the test set. The ordinal logisitic regression method however gives a much larger MSE indicating that this is not a good model to use.\\

The Lasso variable selection method suggests that we ignore the predictors $EthnicBlack$, $EthnicOther$ and $LanguageOther$. The Gibbs variable selection method also suggests we ignore these predictors, however it suggests we ignore ten additional predictors. This would create a simpler model based on less predictors which may be preferable. When we compare the MSE for these two methods we see that Lasso still gives a lower MSE for the test set than the Gibbs variable selection method, despite using a more complicated model for the data. The differences in MSE, between Lasso and Gibbs variable selection, are very small (less than 3\%) and selecting a different randomly sampled test and training set might give slightly different results. \\ 
\end{document}